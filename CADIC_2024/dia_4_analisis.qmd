---
title: | 
  | An√°lisis de datos: modelos lineales habituales en ecolog√≠a
subtitle: "Curso: Programaci√≥n y an√°lisis estad√≠stico en R. CADIC, Ushuaia, Argentina"
author: 
  - name: Ver√≥nica Cruz-Alonso 
    role: "Profesora y autora del material"
  - name: Enrique Andivia
    role: "Autor del material"
date: today
date-format: "DD/MM/YYYY"
toc: true
toc-depth: 4
toc-title: "√çndice"
format:
  html:
    link-external-newwindow: true
    # css: styles.css
  gfm: default
editor: visual
editor_options: 
  chunk_output_type: console
number-sections: true
bibliography: references.bib
---

## Objetivos del d√≠a 5

Los objetivos del d√≠a de hoy son:

-   Aprender los fundamentos de los modelos lineales en R.

-   Aprender qu√© son los modelos generalizados y mixtos y para qu√© se usan.

-   Aprender a reportar los resultados de los modelos.

## Introducci√≥n a los modelos de regresi√≥n

El objetivo de un an√°lisis estad√≠stico es estimar los par√°metros del modelo que conducen al mejor ajuste del mismo a nuestros datos. El mejor modelo es aquel que explica la mayor variabilidad posible de la variable respuesta, siempre que los par√°metros del modelo sean estad√≠sticamente significativos. No existe un √∫nico modelo sino una gran cantidad de modelos que se ajustan a nuestros datos en mayor o menor medida. De entre dos modelos que expliquen la misma variabilidad debemos quedarnos con el m√°s simple de todos (el m√°s parsimonioso).

> All models are wrong, but some are useful
>
> --- George E. P. Box

El **an√°lisis de regresi√≥n** se usa para explicar o predecir la relaci√≥n lineal entre una variable Y (variable respuesta o dependiente), y una o m√°s variables explicativas o independientes. Cuando hay una variable explicativa hablamos de regresi√≥n simple y cuando hay varias es regresi√≥n m√∫ltiple. Si la(s) variable(s) explicativas son categ√≥ricas estamos ante un an√°lisis de la varianza o ANOVA, que al igual que antes puede ser unifactorial si solo hay una variable explicativa (one-way ANOVA) o multifactorial si hay m√°s. Por √∫ltimo, si las variables explicativas son una combinaci√≥n de categ√≥ricas y continuas estaremos antes un ANCOVA (an√°lisis de la covarianza) [@cayuela2022]. Todos estos nombres hacen referencia a tipos de an√°lisis de regresi√≥n que en R se pueden calcular con una √∫nica funci√≥n: `lm(m)`.

## Supuestos de los modelos lineales

La aplicaci√≥n de un modelo lineal queda supeditado al cumplimiento de una serie de supuestos.

-   **Independencia**: las observaciones y, por tanto, los residuos del modelo deben ser independientes entre s√≠. Las formas m√°s comunes de violar el supuesto de independencia son la **autocorrelaci√≥n** espacial, cuando distintas observaciones est√°n m√°s pr√≥ximas entre s√≠ que otras, y la autocorrelaci√≥n temporal cuando se realizan distintas mediciones de la variable dependiente en los mismos individuos o parcelas a lo largo del tiempo. El supuesto de independencia est√° muy relacionado con el dise√±o del estudio. Se puede lidiar con la dependencia de las observaciones promediando el valor de las que sean dependientes entre s√≠, a trav√©s de modelos mixtos que tienen en cuenta la no independencia sin perder la variabilidad asociada a ella, o con modelos autorregresivos.

-   **Normalidad**: los modelos lineales asumen una distribuci√≥n normal de los residuos del modelo. El criterio de normalidad se refiere a los residuos del modelo y no a la variable dependiente, aunque en gran parte la distribuci√≥n de los residuos viene definida por la distribuci√≥n de la variable dependiente. Por eso, una forma de cumplir el supuesto de normalidad es transformar la variable respuesta. Si la variable dependiente son unos y ceros (presencia/ausencia o vivo/muerto), o bien es un conteo (n¬∫ individuos) los residuos no van a ser normales. En estos casos espec√≠ficos utilizaremos modelos lineales generalizados.

-   **Homocedasticidad**: se asume que la varianza de los residuos es homogenea en todo el rango de la variable independiente. Esto implica que la varianza residual de nuestro modelo debe ser constante. La violaci√≥n de este supuesto puede hacernos caer en errores de tipo I (falsos positivos). Para evaluar este aspecto se representan los residuos frente a los valores ajustados por el modelo. Uno de los principales problemas que suele haber es que los residuos aumenten conforme aumentan los valores ajustados por el modelo. Puede deberse a que alguna variable no se ha tenido en cuenta o tambi√©n a la presencia de *outliers* en nuestros datos. Una forma de lidiar con este aspecto es transformar la variable respuesta.

-   **Linealidad**: la relaci√≥n entre la variable dependiente y la independiente es lineal o puede expresarse como una combinaci√≥n lineal de par√°metros.

## Fundamentos matem√°ticos de una regresi√≥n lineal simple

Asumiendo la hip√≥tesis de que *y* se puede explicar como una funci√≥n lineal de *x*, podemos expresar *y* como:

$$
y = a + bx
$$

Al par√°metro *a* se le denomina **intercepto** y es el valor de la variable respuesta cuando la variable independiente es igual a cero. Al par√°metro *b* se le denomina **pendiente** y representa cu√°nto aumenta la variable respuesta por unidad de la variable independiente. Basta estimar los par√°metros *a* y *b* para conocer el valor de la variable respuesta para cada valor de la variable independiente.

Para entender c√≥mo se estiman recordemos que los datos de una regresi√≥n son tomados de manera pareada, es decir que en una misma observaci√≥n tenemos un dato para la variable respuesta (y~i~) y para la variable independiente (x~i~). Sin embargo, los par√°metros *a* y *b* son los mismos para todos los pares de datos por lo que a menos que el modelo prediga con exactitud cada una de las observaciones tendremos un error asociado al modelo. Dicho **error o residuo** (ùúÄ~ùëñ~) es la diferencia entre el valor predicho de la variable respuesta (≈∂~i~) para un determinado valor de la variable independiente y el valor real observado de nuestra variable respuesta (y~i~) en dicha observaci√≥n. Para cada observaci√≥n tendremos:

$$
y_{i} = a + bx_{i} + ùúÄ_{i}
$$

Los ‚Äúmejores‚Äù par√°metros del modelo se estiman mediante **m√°xima verosimilitud**, es decir, dados los datos, el valor de los par√°metros ser√° aquel que haga los datos m√°s veros√≠miles. Si los supuestos de los modelos lineales se cumplen, la m√°xima verosimilitud viene dada por el **m√©todo de los m√≠nimos cuadrados.** El t√©rmino m√≠nimos cuadrados se refiere a la **suma de los residuos al cuadrado** (RSS - Residual Sum of Squares). Los mejores par√°metros del modelo ser√°n aquellos que minimicen la suma de los residuos cuadrados del modelo . La RSS representa la variabilidad que el modelo NO es capaz de explicar.

El **error del modelo** (residuos) es una variable aleatoria que sigue una distribuci√≥n normal, de ah√≠ el supuesto de normalidad, con media igual a cero y varianza ùúé^2^. El valor de la varianza residual de la muestra es lo que se denomina **cuadrados medios residuales** (RMS). La varianza del error estar√° m√°s cercana a cero cuando las observaciones est√©n m√°s cerca de las predicciones. Para calcularla, la RSS se divide por n-2, calculando una especie de valor *medio* de la RSS.

$$RMS = \frac{RSS}{n-2}$$ Asociado a los errores del modelo podemos calcular el grado de ajuste del mismo para lo que se utiliza el **coeficiente de determinaci√≥n** (R^~2~^), que se define como la fracci√≥n de la variabilidad de la variable respuesta explicada por la variaci√≥n de la variable independiente. Si la variabilidad total de la variable respuesta es SSY (la suma de las diferencias entre y~i~ y la media de *y* al cuadrado) y la parte no explicada por el modelo es RSS, definimos el coeficiente de determinaci√≥n como:

$$R^2 = \frac{SSY-RSS}{SSY} $$ Al ser una proporci√≥n, el coeficiente de determinaci√≥n se define entre 0 y 1, cuanto m√°s se acerque a 1 mayor ser√° la variabilidad explicada y mejor se ajustar√° el modelo a los datos.

Los par√°metros del modelo son realmente una estimaci√≥n de los verdaderos valores de los par√°metros poblacionales. Por ello, estos par√°metros llevan asociados una incertidumbre y se necesita **testar estad√≠sticamente si los par√°metros estimados son estad√≠sticamente distintos de cero**. Esto es especialmente importante para la pendiente ya que su significaci√≥n nos indicar√≠a que hay un efecto de la variable independiente sobre la variable respuesta. La hip√≥tesis nula es que *b* no es diferente de cero. La significaci√≥n de estos par√°metros se comprueba mediante un t-test en la funci√≥n `lm()` (eval√∫a la relaci√≥n entre cada variable y la variable respuesta mientras se controlan las dem√°s). Mediante F-ratio con `anova()` se eval√∫a la relaci√≥n entre la variabilidad explicada por el modelo y la variabilidad explicada por esa variable.

```{r lm_simple}

library(tidyverse)

cars |> 
  ggplot() +
  geom_point(aes(x = speed, y = dist)) +
  labs(x = "Velocidad (millas/h)", y = "Distancia de frenado (pies)")

m_cars <- lm(dist ~ speed, data = cars)

summary(m_cars)

# Calculamos a mano la bondad de ajuste del modelo:
res <- resid(m_cars)

summary(res)

RSS <- sum(res^2)
RMS <- RSS/(nrow(cars)-2) 
sqrt(RMS)

SSY <- sum((cars$dist - mean(cars$dist))^2)
R2 <- (SSY-RSS)/SSY

anova(m_cars) 

# La tabla ANOVA tiene una serie de columnas que resumen la partici√≥n de la suma de cuadrados
# La F se calcula bajo el supuesto de b = 0
# Un p-valor de X quiere decir que X de cada 10 veces obtendremos por azar un F-ratio igual al que hemos obtenido con las observaciones.
```

## Ajustando modelos lineales en R

### Regresi√≥n lineal multiple

La regresi√≥n lineal m√∫ltiple es un modelo lineal con dos o m√°s variables independientes continuas. El efecto de las variables explicativas sobre la variable respuesta puede no ser exclusivamente independiente. Pudiera ser que la relaci√≥n entre la variable respuesta y una variable explicativa estuviera modulada por otra variable explicativa y tendr√≠amos una interacci√≥n.

üí°La inclusi√≥n de variables en el modelo aumenta el n√∫mero de par√°metros a estimar. De forma general se suele recomendar tener al menos 10 observaciones independientes por par√°metro a estimar, aunque como siempre esto no debe tomarse a raja tabla.

```{r regresion_multiple}

# install.packages("performance")
library(performance)
# install.packages("")
library(sjPlot)
# install.packages("broom")
library(broom) #https://broom.tidymodels.org/articles/broom.html

ozono <- read_delim(file = "Ozone.txt", delim = "\t") 
ozono

ggpairs(ozono |> select(rad, temp, wind, ozone),
        lower = list(continuous = wrap("smooth", method = "loess", color = "darkslategrey", alpha = 0.1)),
        diag = list(continuous = wrap("barDiag")))  

m_ozono <- lm(ozone ~ rad + temp + wind, data = ozono)
summary(m_ozono)
anova(m_ozono)

# Comprobamos residuos
x11()
check_model(m_ozono)

plot_model(m_ozono, terms = c("rad", "temp", "wind"), show.data = TRUE, type = "resid")

# Modelo con interacciones 

ozono <- ozono %>% 
  mutate(rads = as.vector(scale(rad)),
    temps = as.vector(scale(temp)),
    winds = as.vector(scale(wind)))

m_ozono_int <- lm(ozone ~ rad*temp + wind*temp, data = ozono)
summary(m_ozono_int)
anova(m_ozono_int)
confint(m_ozono_int)

# comprobar supuestos
x11()
check_model(m_ozono_int)

plot_model(m_ozono_int, terms = c("rad", "temp", "wind"), show.data = TRUE, type = "resid")

# Para representar las predicciones del gr√°fico

plot_model(m_ozono_int, type = "pred",
  terms = c("temp","rad[90, 270]")) +
  labs(title = "", 
    x = "Temperature", y = "Ozone concentration", color = "Radiation") + 
  theme_bw()

plot_model(m_ozono_int, type = "pred",
  terms = c("temp","wind[7, 13]")) +
  labs(title = "", 
    x = "Temperature", y = "Ozone concentration", color = "Wind") +
  theme_bw()

# Tablas de resultados

tidy(m_ozono_int)
tidy(anova(m_ozono_int))
glance(m_ozono_int)

```

### ANOVA (ANalysis Of VAriance)

Al calcular un ANOVA ajustamos un modelo lineal utilizando variables dummies (es decir, que toman valores de 0 o 1). El modelo ajusta tantas pendientes como n ‚Äì 1 niveles del factor existan. El intercepto toma el valor de la variable respuesta para el nivel de referencia del factor (R lo identifica por orden alfab√©tico). El valor de los par√°metros estimado para el resto de niveles nos indica la diferencia que existe entre el valor medio de la variable respuesta para dichos niveles y el del nivel de referencia.

```{r anova}

# install.packages("emmeans")
library(emmeans)
# Guia de emmeans: https://rvlenth.github.io/emmeans/articles/AQuickStart.html

pinos <- read_delim(file = "RxF_growth.csv", delim = ",",
           col_types = list(Irrig = "f",
             Fert = "f"))

# pinos contiene datos de un experimento en el que se plantaron 144 pl√°ntulas de Pinus pinea en un campo abandonado y se sometieron a un experimento factorial con 4 niveles de riego (0, 150, 300 y 600 mm a√±o‚Åª¬π) y 3 tratamientos de fertilizaci√≥n nitrogenada (0, 150 y 300 kg N ha‚Åª¬π) distribuidos aleatoriamente.

# Queremos saber si el incremento en diam√©tro (DI) estuvo condicionado por la irrigaci√≥n

pinos

# Exploraci√≥n de las variables

# Outliers

ggplot(pinos) +
  geom_boxplot(aes(y = DI)) 
ggplot(pinos) +
  geom_jitter(aes(y = DI, x = 1)) 
# considero que no hay

# Normalidad y homogeneidad

ggplot(pinos) + #normal
  geom_histogram(aes(x = DI)) 
ggplot(pinos) +
  geom_boxplot(aes(x = Irrig, y = DI)) 

# Ajustamos un modelo lineal

m_pinos <- lm(DI ~ Irrig, data = pinos)
summary(m_pinos)

# ¬øQu√© significa cada par√°metro?
# intercept = media del nivel de referencia, variables dummy

pinos <- pinos |> 
  mutate(f1 = fitted(m_pinos))

ggplot(pinos) +
  geom_point(aes(x = f1, y = DI)) +
  labs(y = "Observados", x = "Predichos")

# Tabla del anova

anova(m_pinos)

# Irrig SumSq / (Irrig SumSq + Residuals SumSq) = R2 
3259.6/(3259.6+6744.8)

# Check residuals
x11()
check_model(m1)

# Pero... no hemos acabado
# Una de las principales razones de usar factores es conocer las diferencias entre los niveles del factor

paircomp <- emmeans::emmeans(m_pinos, specs = pairwise ~ Irrig)
paircomp

# Representaci√≥n del modelo

plot_model(m_pinos, type = "pred", terms = "Irrig", show.data = TRUE, jitter = 0.5) +
  labs(title = "", 
    x = "Tratamiento de riego", y = "Incremento en di√°metro") +
  theme_bw()

plot(paircomp, comparison = TRUE) +
  labs(title = "", 
    x = "Tratamiento de riego", y = "Incremento en di√°metro") +
  theme_bw()

# Tablas de resultados

tidy(m_pinos)
tidy(anova(m_pinos))
glance(m_pinos)
```

### ANCOVA

```{r ancova}

encinas <- read_delim(file = "bellotas.csv", delim = ",",
           col_types = list(Competition = "f",
             Fert = "f"))

# encinas contiene informaci√≥n de 40 √°rboles adultos de encina (Quercus ilex). En alrededor de la mitad de ellos se eliminaron todas las plantas competidoras (herb√°ceas), dando lugar a dos tratamientos de competencia (s√≠ y no). La mitad de las encinas en cada tratamiento de competencia se sometieron a fertilizaci√≥n durante tres a√±os, mientras que la otra mitad se dej√≥ como control. Despu√©s de 3 a√±os, se midi√≥ el di√°metro y se calcul√≥ el crecimiento como el aumento en di√°metro (DI). La variable de respuesta fue la producci√≥n de bellotas por √°rbol.

# El nombre "Acorn " tiene un espacio
encinas <- encinas |> 
  rename(Acorn = `Acorn `)

# La hip√≥tesis es que aquellos √°rboles que crezcan m√°s tambi√©n mostrar√°n una mayor producci√≥n de bellotas y que esta relaci√≥n depender√° de la disponibilidad de recursos, es decir, m√°s nutrientes y agua debido a la fertilizaci√≥n y la falta de competencia.

# Comprobaciones: outliers, normalidad, linealidad

ggplot(encinas) +
  geom_boxplot(aes(y = Acorn, x = 1)) 

ggplot(encinas) +
  geom_boxplot(aes(y = Acorn, x = Fert))

ggplot(data = encinas) +
  geom_boxplot(aes(y = Acorn, x = Competition))

ggplot(encinas) +
  geom_point(aes(x = Acorn, y = DI))

ggplot(encinas) +
  geom_histogram(aes(x = Acorn), bins = 10) 

# Ajustamos el modelo

m_encinas <- lm(Acorn ~ Fert * DI + Competition * DI, data = encinas)

summary(m_encinas)
anova(m_encinas)
check_model(m_encinas)
# ¬øQuitar√≠ais la interacci√≥n Fert * DI?

m_encinas2 <- lm(Acorn ~ Fert + Competition * DI, data = encinas)

summary(m_encinas2)
anova(m_encinas2)
check_model(m_encinas2)

# Interpretaci√≥n de los par√°metros

# Figura con la relaci√≥n entre las variables continuas para cada nivel del tratamiento
plot_model(m_encinas2, type = "pred", terms = c("DI","Competition")) +
  labs(title = "", 
    x = "Crecimiento", y = "Producci√≥n de bellotas", color = "Competencia") +
  theme_bw()


```

### Colinealidad en modelos lineales

VIF root of 1 / (1 Rj ), also called the variance inflation factor (VIF), whichmeans that the P-values get largermaking it more difficult to detect an effect El factor de inflaci√≥n de la varianza proporciona una estimaci√≥n cuantitativa de la multicolinealidad entre covariables en un an√°lisis de regresi√≥n. Este √≠ndice mide hasta qu√© punto la varianza de un coeficiente de regresi√≥n estimado se incrementa a causa de la colinealidad. Si echamos un vistazo a la f√≥rmula mediante la que se calcula la varianza asociada a la estimaci√≥n de un par√°metro mediante m√≠nimos cuadrados veremos que a mayor R2 (m√°s cercano a 1) mayor varianza asociada al par√°metros. Hay que aclarar que este R2 es el cuadrado del coeficiente de correlaci√≥n m√∫ltiple entre las variables independientes del modelo y no el coeficiente de determinaci√≥n del mismo. En t√©rminos pr√°cticos, si introducimos una variable en el modelo que aumenta la correlaci√≥n entre las mismas significa que la informaci√≥n proporcionada por esta variable ya est√° incluida en el resto y adem√°s aumentamos la varianza asociada a la estimaci√≥n de nuestros par√°metros por lo que podemos estar influyendo la significaci√≥n de los mismos. En cuanto a los valores de VIF a partir de los cuales hay que tomar en serio la colinealidad entre nuestras variables no existe un consenso claro. El l√≠mite m√°s aceptado es el de 5, aunque algunos autores establecen este l√≠mite en 10. Sin embargo cuando nuestras variables no tienen un efecto muy marcada sobre nuestra variable respuesta, tener valores de VIF mayores de 3 podr√≠a suponer un problema ya que muchos de los par√°metros podr√≠an dejar de ser significativos. El procedimiento a seguir es calcular el VIF sobre las variables independientes presentes en el modelo y en caso de alta colinealidad eliminar aquella con un VIF superior y volver a calcular el VIF para cada covariable.

## Modelos lineales mixtos

Los modelos mixtos estiman par√°metros (interceptos o pendientes) que var√≠an seg√∫n unos niveles de agregaci√≥n especificados en vez de considerarse plenamente independientes. Ahora el intercepto *a* o la pendiente *b* van a seguir una distribuci√≥n normal que depende de otro/s predictor/es:

$$
a_{j} \sim N(ùúá_{j}, ùúé^2)\\
ùúá_{j}=ùõæ+ùõø√ópredictor_{j}
$$

Este tipo de modelos es muy √∫til para modelizar datos estructurados en el espacio o en el tiempo (medidas repetidas) a la vez que permiten aprovechar todos los datos.

üí°Si te interesan los modelos mixtos, puedes leer el cap√≠tulo *Multilevel linear models: the basics* de @gelman2006.

```{r m_mixtos}
# install.packages("glmmTMB")
library(glmmTMB)
# install.packages("lmerTest")
library(lmerTest)
# install.packages("broom.mixed")
library(broom.mixed)
# install.packages("DHARMa")
library(DHARMa)

library(palmerpenguins)

# Un solo grupo de datos

lm(flipper_length_mm ~ body_mass_g, data = penguins)

# Sin agrupamiento: un intercepto independiente por grupo.

lm(flipper_length_mm ~ body_mass_g + factor(year), data = penguins)

# Con agrupamiento parcial: interceptos interrelacionados

glmmTMB(flipper_length_mm ~ body_mass_g + (1|year), data = penguins)

# Ajustamos modelo mixto

ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, color = factor(year))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm")

m_mixto <- glmmTMB(flipper_length_mm ~ body_mass_g + (1|year), data = penguins)

summary(m_mixto)
check_model(m_mixto)

tidy(m_mixto)

plot_model(m_mixto, terms = c("body_mass_g"), show.data = TRUE, type = "pred")
plot_model(m_mixto, terms = c("year"), show.data = TRUE, type = "re")

# + (1 | group) varian interceptos
# + (1 + x | group) varian interceptos y pendientes
# + (1 | group/subgroup) varian interceptos anidados
# + (1 | group1) + (1 | group2) # varian interceptos seg√∫n 2 grupos independientes
# + (1 + x | group1) + (1 + x | group2) # varian interceptos y pendientes seg√∫n dos grupos

#### 1. Conteo Poisson ####

# Importar datos
# Base de datos para estudiar el efecto de la composicion forestal y la disponibilidad
# de alimento sobre la abundacia de la ardilla roja en Escocia
# SqCones= n pinas comidas (variable dep)
# Ntrees= n trees per plot; DBH= mean tree DBH; TreeHeight= mean tree height; CanopyCover
SQ <- read_delim(file = "RedSquirrels.txt", delim = "\t") 
SQ

# Exploratory
outliers <- list(NULL)
for (i in 2:6){
  midf <- data.frame(a = SQ[[i]])
  outliers[[i-1]] <- ggplot(midf) +
    geom_boxplot(aes(y = a)) +
    labs(y = names(SQ)[i], x = "")
}

outliersplot <- patchwork::wrap_plots(outliers, nrow = 2)
outliersplot

# outlier en DBH
SQ2 <- SQ |> 
  filter(DBH < 0.7)

ggpairs(
  SQ |> select(SqCones, Ntrees, DBH, TreeHeight, CanopyCover),
  lower = list(
    continuous = wrap("smooth", method = "loess", color = "darkslategrey", alpha = 0.1)),
  diag = list(continuous = wrap("barDiag"))
)

SQ2 <- SQ2 %>% 
  mutate(Ntreess = as.vector(scale(Ntrees)),
         TreeHeights = as.vector(scale(TreeHeight)),
         CanopyCovers = as.vector(scale(CanopyCover)),
         DBHs = as.vector(scale(DBH)))

summary(SQ2)

# Modelo
mod <- lm(SqCones ~ Ntreess + DBHs + TreeHeights + CanopyCovers, 
  data = SQ2)
summary(mod)
check_model(mod)

# Ajustamos el modelo (conteo - poisson)
M1 <- glm(SqCones ~ Ntreess + DBHs + TreeHeights + CanopyCovers,
  family = "poisson", data = SQ2)
summary(M1)
(882.20 - 647.35) / 882.20 # devianza explicada

# Predicciones (Para una de las covariables)
plot_model(M1, terms = "Ntreess", show.data = TRUE, type = "pred")
plot_model(M1, terms = "DBHs", show.data = TRUE, type = "pred")
plot_model(M1, terms = "TreeHeights", show.data = TRUE, type = "pred")
plot_model(M1, terms = "CanopyCovers", show.data = TRUE, type = "pred")

# Check overdispersion 
summary(M1)
647.35/46
dispersiontest(M1)
check_overdispersion(M1)

# Zero-inflated, missing covariate, wrong error distribution, wrong link function???
x11()
check_model(M1)

# Residuals vs fitted
# Hay dos observaciones que influyen mucho y muchas cercanas a 1, demasiadas para quitarlas
# Cuando hay sobredispersion en poisson se suele usar una binomial negativa
M2 <- MASS::glm.nb(
  SqCones ~ Ntreess + DBHs + TreeHeights + CanopyCovers,
  data = SQ2,
  na.action = na.fail
)
summary(M2)

# Hay un nuevo parametro para la varianza theta
59.027/46 # Sin sobredispersi√≥n

x11()
check_model(M2)

# Model selection
dredge(M2)
M3 <- MASS::glm.nb(SqCones ~ CanopyCovers, data = SQ2, na.action = na.fail)

# Predicciones
p <- plot_model(M3, type = "pred", terms = c("CanopyCovers")) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha=0.3, fill="purple3")+
  geom_line(size = 0.8) +
  geom_point(data = SQ2, aes(y = SqCones, x = CanopyCovers), alpha = 0.5) +
  labs(y = "N¬∫ de pi√±as comidas", x = "Cobertura de copa") +
  theme_bw() +
  theme(plot.title = element_blank(),
        axis.title = element_text(size = 16, 
                                  face = "bold"),
        axis.text = element_text(size = 14)) +
  theme(panel.grid = element_blank())

p

ggsave(filename="miGGplot.jpg", dpi = 1200, 
       plot = p, width = 100, height = 100, 
       units = "mm")

#### 2. Modelo presencia-ausencia ####

# Importar datos
# Heteromastus similis es un gusano marino, 
# modelamos su presencia (Hsimilis) en funcion de
# MedSand= porcentaje de arena
# Level = preferencia profundidad sustrato 
# Location= 3 areas con distintas condiciones ambientales
PO <- read_delim(file = "PolychaetaV3.txt", delim = "\t",
  col_types = list(Level = "f",
    Location = "f"))

PO

# Modelo
M1 <- glm(Hsimilis ~ MedSand * Level, data = PO, family = "binomial")
summary(M1)
M1.1 <- glm(Hsimilis ~ MedSand + Level, data = PO, family = "binomial")
AIC(M1, M1.1)
# Validacion modelo
x11()
check_model(M1)

# Predicciones
range(PO$MedSand)

plot_model(M1, terms = c("MedSand","Level"), show.data = TRUE, type = "pred")
plot_model(M1, terms = c("Level"), show.data = TRUE, type = "pred")



# Con efecto sobre la pendiente
M3 <- lme(Richness ~ NAP, random = ~NAP|Beach, data = rikz)
summary(M3)
anova(M3)

M3.1 <- lme(Richness ~ NAP, random = ~NAP|Beach, data = rikz, method = "REML")
AIC(M2.1, M3.1)

# Residuos
check_model(M3)

M3.2 <- lmer(Richness ~ NAP + (NAP|Beach), data = rikz)
check_model(M3.2)

#### 4. Modelo mixto generalizado ####

M4 <- glmer(Richness ~ NAP + (NAP|Beach), data = rikz, family=poisson)
summary(M4)

x11()
check_model(M4)

plot_model(M4, terms = "NAP", show.data = TRUE, type = "pred")

# M√∫ltiples covariables - distancias de Cook

ggplot(data = wclam) + 
  geom_point(aes(x = log10(LENGTH), y = log10(AFD))) +
  labs(y = "Peso", x = "Longitud") 

m1 <- lm(log10(AFD) ~ log10(LENGTH), data = wclam) 
m2 <- lm(log10(AFD) ~ log10(LENGTH), data = wclam |> slice(-108)) # Otro sin 108

plot(m1, which = 4) # mayor de 1
plot(m2, which = 4)

check_outliers(m1)
x11()
check_outliers(m1) |> plot()
check_outliers(m2)

```

## Modelos lineales generalizados
