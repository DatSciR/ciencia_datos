---
title: | 
  | Análisis de datos: modelos lineales habituales en ecología
subtitle: "Curso: Programación y análisis estadístico en R. CADIC, Ushuaia, Argentina"
author: 
  - name: Verónica Cruz-Alonso 
    role: "Profesora y autora del material"
  - name: Enrique Andivia
    role: "Autor del material"
date: today
date-format: "DD/MM/YYYY"
toc: true
toc-depth: 4
toc-title: "Índice"
format:
  html:
    link-external-newwindow: true
    # css: styles.css
  gfm: default
editor: visual
editor_options: 
  chunk_output_type: console
number-sections: true
bibliography: references.bib
---

## Objetivos del día 4

Los objetivos del día de hoy son:

-   Aprender los fundamentos de los modelos lineales en R.

-   Aprender qué son los modelos generalizados y mixtos y para qué se usan.

-   Aprender a reportar los resultados de los modelos.

## Introducción a los modelos de regresión

El objetivo de un análisis estadístico es estimar los parámetros del modelo que conducen al mejor ajuste del mismo a nuestros datos. El mejor modelo es aquel que explica la mayor variabilidad posible de la variable respuesta, siempre que los parámetros del modelo sean estadísticamente significativos. No existe un único modelo sino una gran cantidad de modelos que se ajustan a nuestros datos en mayor o menor medida. De entre dos modelos que expliquen la misma variabilidad debemos quedarnos con el más simple de todos (el más parsimonioso).

> All models are wrong, but some are useful
>
> --- George E. P. Box

El **análisis de regresión** se usa para explicar o predecir la relación lineal entre una variable Y (variable respuesta o dependiente), y una o más variables explicativas o independientes. Cuando hay una variable explicativa hablamos de regresión simple y cuando hay varias es regresión múltiple. Si la(s) variable(s) explicativas son categóricas estamos ante un análisis de la varianza o ANOVA, que al igual que antes puede ser unifactorial si solo hay una variable explicativa (one-way ANOVA) o multifactorial si hay más. Por último, si las variables explicativas son una combinación de categóricas y continuas estaremos antes un ANCOVA (análisis de la covarianza) [@cayuela2022]. Todos estos nombres hacen referencia a tipos de análisis de regresión que en R se pueden calcular con una única función: `lm()`.

## Supuestos de los modelos lineales

La aplicación de un modelo lineal queda supeditado al cumplimiento de una serie de supuestos.

-   **Independencia**: las observaciones y, por tanto, los residuos del modelo deben ser independientes entre sí. Las formas más comunes de violar el supuesto de independencia son la **autocorrelación** espacial, cuando distintas observaciones están más próximas entre sí que otras, y la autocorrelación temporal cuando se realizan distintas mediciones de la variable dependiente en los mismos individuos o parcelas a lo largo del tiempo. El supuesto de independencia está muy relacionado con el diseño del estudio. Se puede lidiar con la dependencia de las observaciones promediando el valor de las que sean dependientes entre sí, a través de modelos mixtos que tienen en cuenta la no independencia sin perder la variabilidad asociada a ella, o con modelos autorregresivos.

-   **Normalidad**: los modelos lineales asumen una distribución normal de los residuos del modelo. El criterio de normalidad se refiere a los residuos del modelo y no a la variable dependiente, aunque en gran parte la distribución de los residuos viene definida por la distribución de la variable dependiente. Por eso, una forma de cumplir el supuesto de normalidad es transformar la variable respuesta. Si la variable dependiente son unos y ceros (presencia/ausencia o vivo/muerto), o bien es un conteo (nº individuos) los residuos no van a ser normales. En estos casos específicos utilizaremos modelos lineales generalizados.

-   **Homocedasticidad**: se asume que la varianza de los residuos es homogenea en todo el rango de la variable independiente. Esto implica que la varianza residual de nuestro modelo debe ser constante. La violación de este supuesto puede hacernos caer en errores de tipo I (falsos positivos). Para evaluar este aspecto se representan los residuos frente a los valores ajustados por el modelo. Uno de los principales problemas que suele haber es que los residuos aumenten conforme aumentan los valores ajustados por el modelo. Puede deberse a que alguna variable no se ha tenido en cuenta o también a la presencia de *outliers* en nuestros datos. Una forma de lidiar con este aspecto es transformar la variable respuesta.

-   **Linealidad**: la relación entre la variable dependiente y la independiente es lineal o puede expresarse como una combinación lineal de parámetros.

## Fundamentos matemáticos de una regresión lineal simple

Asumiendo la hipótesis de que *y* se puede explicar como una función lineal de *x*, podemos expresar *y* como:

$$
y = a + bx
$$

Al parámetro *a* se le denomina **intercepto** y es el valor de la variable respuesta cuando la variable independiente es igual a cero. Al parámetro *b* se le denomina **pendiente** y representa cuánto aumenta la variable respuesta por unidad de la variable independiente. Basta estimar los parámetros *a* y *b* para conocer el valor de la variable respuesta para cada valor de la variable independiente.

Para entender cómo se estiman recordemos que los datos de una regresión son tomados de manera pareada, es decir que en una misma observación tenemos un dato para la variable respuesta (y~i~) y para la variable independiente (x~i~). Sin embargo, los parámetros *a* y *b* son los mismos para todos los pares de datos por lo que a menos que el modelo prediga con exactitud cada una de las observaciones tendremos un error asociado al modelo. Dicho **error o residuo** (𝜀~𝑖~) es la diferencia entre el valor predicho de la variable respuesta (Ŷ~i~) para un determinado valor de la variable independiente y el valor real observado de nuestra variable respuesta (y~i~) en dicha observación. Para cada observación tendremos:

$$
y_{i} = a + bx_{i} + 𝜀_{i}
$$

Los “mejores” parámetros del modelo se estiman mediante **máxima verosimilitud**, es decir, dados los datos, el valor de los parámetros será aquel que haga los datos más verosímiles. Si los supuestos de los modelos lineales se cumplen, la máxima verosimilitud viene dada por el **método de los mínimos cuadrados.** El término mínimos cuadrados se refiere a la **suma de los residuos al cuadrado** (RSS - Residual Sum of Squares). Los mejores parámetros del modelo serán aquellos que minimicen la suma de los residuos cuadrados del modelo . La RSS representa la variabilidad que el modelo NO es capaz de explicar.

El **error del modelo** (residuos) es una variable aleatoria que sigue una distribución normal, de ahí el supuesto de normalidad, con media igual a cero y varianza 𝜎^2^. El valor de la varianza residual de la muestra es lo que se denomina **cuadrados medios residuales** (RMS). La varianza del error estará más cercana a cero cuando las observaciones estén más cerca de las predicciones. Para calcularla, la RSS se divide por n-2, calculando una especie de valor *medio* de la RSS.

$$RMS = \frac{RSS}{n-2}$$ Asociado a los errores del modelo podemos calcular el grado de ajuste del mismo para lo que se utiliza el **coeficiente de determinación** (R^~2~^), que se define como la fracción de la variabilidad de la variable respuesta explicada por la variación de la variable independiente. Si la variabilidad total de la variable respuesta es SSY (la suma de las diferencias entre y~i~ y la media de *y* al cuadrado) y la parte no explicada por el modelo es RSS, definimos el coeficiente de determinación como:

$$R^2 = \frac{SSY-RSS}{SSY} $$ Al ser una proporción, el coeficiente de determinación se define entre 0 y 1, cuanto más se acerque a 1 mayor será la variabilidad explicada y mejor se ajustará el modelo a los datos.

Los parámetros del modelo son realmente una estimación de los verdaderos valores de los parámetros poblacionales. Por ello, estos parámetros llevan asociados una incertidumbre y se necesita **testar estadísticamente si los parámetros estimados son estadísticamente distintos de cero**. Esto es especialmente importante para la pendiente ya que su significación nos indicaría que hay un efecto de la variable independiente sobre la variable respuesta. La hipótesis nula es que *b* no es diferente de cero. La significación de estos parámetros se comprueba mediante un t-test en la función `lm()` (evalúa la relación entre cada variable y la variable respuesta mientras se controlan las demás). Mediante F-ratio con `anova()` se evalúa la relación entre la variabilidad explicada por el modelo y la variabilidad explicada por esa variable.

```{r lm_simple}
#| warning: false

library(tidyverse)

cars |> 
  ggplot() +
  geom_point(aes(x = speed, y = dist)) +
  labs(x = "Velocidad (millas/h)", y = "Distancia de frenado (pies)")

m_cars <- lm(dist ~ speed, data = cars)

summary(m_cars)

# Calculamos a mano la bondad de ajuste del modelo:
res <- resid(m_cars)

summary(res)

RSS <- sum(res^2)
RMS <- RSS/(nrow(cars)-2) 
sqrt(RMS)

SSY <- sum((cars$dist - mean(cars$dist))^2)
R2 <- (SSY-RSS)/SSY

anova(m_cars) 

# La tabla ANOVA tiene una serie de columnas que resumen la partición de la suma de cuadrados
# La F se calcula bajo el supuesto de b = 0
# Un p-valor de X quiere decir que X de cada 10 veces obtendremos por azar un F-ratio igual al que hemos obtenido con las observaciones.
```

## Ajustando modelos lineales en R

### Regresión lineal multiple

La regresión lineal múltiple es un modelo lineal con dos o más variables independientes continuas. El efecto de las variables explicativas sobre la variable respuesta puede no ser exclusivamente independiente. Pudiera ser que la relación entre la variable respuesta y una variable explicativa estuviera modulada por otra variable explicativa y tendríamos una interacción.

💡La inclusión de variables en el modelo aumenta el número de parámetros a estimar. De forma general se suele recomendar tener al menos 10 observaciones independientes por parámetro a estimar, aunque como siempre esto no debe tomarse a raja tabla.

```{r regresion_multiple}
#| warning: false

# install.packages("performance")
library(performance)
# install.packages("")
library(sjPlot)
# install.packages("broom")
library(broom) # https://broom.tidymodels.org/articles/broom.html
library(GGally)

ozono <- read_delim(file = "Ozone.txt", delim = "\t") 
ozono

ggpairs(ozono |> select(rad, temp, wind, ozone),
        lower = list(continuous = wrap("smooth", method = "loess", color = "darkslategrey", alpha = 0.1)),
        diag = list(continuous = wrap("barDiag")))  

m_ozono <- lm(ozone ~ rad + temp + wind, data = ozono)
summary(m_ozono)
anova(m_ozono)

# Comprobamos residuos
check_model(m_ozono)

# Comprobamos residuos parciales
plot_model(m_ozono, terms = c("rad", "temp", "wind"), show.data = TRUE, type = "resid")

# Comprobamos colinealidad
check_collinearity(m_ozono)

# El factor de inflación de la varianza (VIF) proporciona una estimación cuantitativa de la multicolinealidad entre covariables en un análisis de regresión. Este índice mide hasta qué punto la varianza de un coeficiente de regresión se incrementa a causa de la colinealidad. En cuanto a los valores de VIF a partir de los cuales hay que tomar en serio la colinealidad no existe un consenso claro (5 o 10). En caso de alta colinealidad se puede eliminar la variable con un VIF superior y volver a calcular el VIF.

# Modelo con interacciones 

ozono <- ozono |> 
  mutate(rad = as.vector(scale(rad)),
    temp = as.vector(scale(temp)),
    wind = as.vector(scale(wind)))

m_ozono_int <- lm(ozone ~ rad*temp + wind*temp, data = ozono)
summary(m_ozono_int)
anova(m_ozono_int)
confint(m_ozono_int)

# comprobar supuestos
check_model(m_ozono_int)

plot_model(m_ozono_int, terms = c("rad", "temp", "wind"), show.data = TRUE, type = "resid")

# Para representar las predicciones del gráfico

plot_model(m_ozono_int, type = "pred",
  terms = c("temp","rad[90, 270]")) +
  labs(title = "", 
    x = "Temperature", y = "Ozone concentration", color = "Radiation") + 
  theme_bw()

plot_model(m_ozono_int, type = "pred",
  terms = c("temp","wind[7, 13]")) +
  labs(title = "", 
    x = "Temperature", y = "Ozone concentration", color = "Wind") +
  theme_bw()

# Tablas de resultados

tidy(m_ozono_int) 
tidy(anova(m_ozono_int))
glance(m_ozono_int)

```

### ANOVA (ANalysis Of VAriance)

Al calcular un ANOVA ajustamos un modelo lineal utilizando variables dummies (es decir, que toman valores de 0 o 1). El modelo ajusta tantas pendientes como n – 1 niveles del factor existan. El intercepto toma el valor de la variable respuesta para el nivel de referencia del factor (R lo identifica por orden alfabético). El valor de los parámetros estimado para el resto de niveles nos indica la diferencia que existe entre el valor medio de la variable respuesta para dichos niveles y el del nivel de referencia.

```{r anova}
#| warning: false

# install.packages("emmeans")
library(emmeans)
# Guia de emmeans: https://rvlenth.github.io/emmeans/articles/AQuickStart.html

pinos <- read_delim(file = "RxF_growth.csv", delim = ",",
           col_types = list(Irrig = "f",
             Fert = "f"))

# pinos contiene datos de un experimento en el que se plantaron 144 plántulas de Pinus pinea en un campo abandonado y se sometieron a un experimento factorial con 4 niveles de riego (0, 150, 300 y 600 mm año⁻¹) y 3 tratamientos de fertilización nitrogenada (0, 150 y 300 kg N ha⁻¹) distribuidos aleatoriamente.

# Queremos saber si el incremento en diamétro (DI) estuvo condicionado por la irrigación

pinos

# Exploración de las variables

# Outliers

ggplot(pinos) +
  geom_boxplot(aes(y = DI)) 
ggplot(pinos) +
  geom_jitter(aes(y = DI, x = 1)) 
# considero que no hay

# Normalidad y homogeneidad

ggplot(pinos) + #normal
  geom_histogram(aes(x = DI)) 
ggplot(pinos) +
  geom_boxplot(aes(x = Irrig, y = DI)) 

# Ajustamos un modelo lineal

m_pinos <- lm(DI ~ Irrig, data = pinos)
summary(m_pinos)

# ¿Qué significa cada parámetro?
# intercept = media del nivel de referencia, variables dummy

pinos <- pinos |> 
  mutate(f1 = fitted(m_pinos))

ggplot(pinos) +
  geom_point(aes(x = f1, y = DI)) +
  labs(y = "Observados", x = "Predichos")

# Tabla del anova

anova(m_pinos)

# Irrig SumSq / (Irrig SumSq + Residuals SumSq) = R2 
3259.6/(3259.6+6744.8)

# Check residuals
x11()
check_model(m_pinos)

# Pero... no hemos acabado
# Una de las principales razones de usar factores es conocer las diferencias entre los niveles del factor

paircomp <- emmeans(m_pinos, specs = pairwise ~ Irrig)
paircomp

# Representación del modelo

plot_model(m_pinos, type = "pred", terms = "Irrig", show.data = TRUE, jitter = 0.5, alpha = 0.2) +
  labs(title = "", 
    x = "Tratamiento de riego", y = "Incremento en diámetro") +
  theme_bw()

plot(paircomp, comparison = TRUE) +
  labs(title = "", 
    x = "Tratamiento de riego", y = "Incremento en diámetro") +
  theme_bw()

# Tablas de resultados

tidy(m_pinos)
tidy(anova(m_pinos))
glance(m_pinos)
```

### ANCOVA

```{r ancova}
#| warning: false

encinas <- read_delim(file = "bellotas.csv", delim = ",",
           col_types = list(Competition = "f",
             Fert = "f"))

# encinas contiene información de 40 árboles adultos de encina (Quercus ilex). En alrededor de la mitad de ellos se eliminaron todas las plantas competidoras (herbáceas), dando lugar a dos tratamientos de competencia (sí y no). La mitad de las encinas en cada tratamiento de competencia se sometieron a fertilización durante tres años, mientras que la otra mitad se dejó como control. Después de 3 años, se midió el diámetro y se calculó el crecimiento como el aumento en diámetro (DI). La variable de respuesta fue la producción de bellotas por árbol.

# El nombre "Acorn " tiene un espacio
encinas <- encinas |> 
  rename(Acorn = `Acorn `)

# La hipótesis es que aquellos árboles que crezcan más también mostrarán una mayor producción de bellotas y que esta relación dependerá de la disponibilidad de recursos, es decir, más nutrientes y agua debido a la fertilización y la falta de competencia.

# Comprobaciones: outliers, normalidad, linealidad

ggplot(encinas) +
  geom_boxplot(aes(y = Acorn, x = 1)) 

ggplot(encinas) +
  geom_boxplot(aes(y = Acorn, x = Fert))

ggplot(data = encinas) +
  geom_boxplot(aes(y = Acorn, x = Competition))

ggplot(encinas) +
  geom_point(aes(x = Acorn, y = DI))

ggplot(encinas) +
  geom_histogram(aes(x = Acorn), bins = 10) 

# Ajustamos el modelo

m_encinas <- lm(Acorn ~ Fert * DI + Competition * DI, data = encinas)

summary(m_encinas)
anova(m_encinas)
check_model(m_encinas)
# ¿Quitaríais la interacción Fert * DI?

m_encinas2 <- lm(Acorn ~ Fert + Competition * DI, data = encinas)

summary(m_encinas2)
anova(m_encinas2)
check_model(m_encinas2)

# Interpretación de los parámetros

# Figura con la relación entre las variables continuas para cada nivel del tratamiento
plot_model(m_encinas2, type = "pred", terms = c("DI","Competition")) +
  labs(title = "", 
    x = "Crecimiento", y = "Producción de bellotas", color = "Competencia") +
  theme_bw()


```

## Modelos lineales mixtos

Los modelos mixtos estiman parámetros (interceptos o pendientes) que varían según unos niveles de agregación especificados en vez de considerarse plenamente independientes. Ahora el intercepto *a* o la pendiente *b* van a seguir una distribución normal que depende de otro/s predictor/es:

$$
a_{j} \sim N(𝜇_{j}, 𝜎^2)\\
𝜇_{j}=𝛾+𝛿×predictor_{j}
$$

Este tipo de modelos es muy útil para modelizar datos estructurados en el espacio o en el tiempo (medidas repetidas) a la vez que permiten aprovechar todos los datos. No se recomienda ajustar modelos mixtos cuando hay menos de 5-6 niveles del factor y 10-20 observaciones por nivel [@crawley2007; @bolker2009].

💡Si te interesan los modelos mixtos, puedes leer el capítulo *Multilevel linear models: the basics* de @gelman2006.

```{r m_mixtos}
#| warning: false

# install.packages("glmmTMB")
library(glmmTMB)

# install.packages("broom.mixed")
library(broom.mixed)

library(palmerpenguins)

# Un solo grupo de datos

lm(flipper_length_mm ~ body_mass_g, data = penguins)

# Sin agrupamiento: un intercepto independiente por grupo.

lm(flipper_length_mm ~ body_mass_g + factor(year), data = penguins)

# Con agrupamiento parcial: interceptos interrelacionados

glmmTMB(flipper_length_mm ~ body_mass_g + (1|year), data = penguins)

# Ojo, esto es solo un ejemplo. No se recomienda ajustar modelos mixtos cuando hay menos de 5-6 niveles del factor. 

# Ajustamos modelo mixto

ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, color = factor(year))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm")

m_mixto <- glmmTMB(flipper_length_mm ~ body_mass_g + (1|year), data = penguins)

summary(m_mixto)
check_model(m_mixto)

tidy(m_mixto)

plot_model(m_mixto, terms = c("body_mass_g"), show.data = TRUE, type = "pred")
plot_model(m_mixto, terms = c("year"), show.data = TRUE, type = "re")

# + (1 | group) varian interceptos
# + (1 + x | group) varian interceptos y pendientes
# + (1 | group/subgroup) varian interceptos anidados
# + (1 | group1) + (1 | group2) # varian interceptos según 2 grupos independientes
# + (1 + x | group1) + (1 + x | group2) # varian interceptos y pendientes según dos grupos

```

## Modelos lineales generalizados

Los residuos dependen en gran medida de la naturaleza de la variable respuesta. ¿Que ocurre cuando no son normales ni la varianza homogénea? En ecología esto ocurre a menudo con variables binarias (0/1; datos de supervivencia, presencia-ausencia, sexo, etc.) y con conteos (0, 1, 2, 3...; nº de individiuos, nº de avistamientos, etc.).

```{r generalizados}
#| warning: false

library(titanic)

titanic <- titanic_train |> 
  mutate(Pclass = factor(Pclass))


m_lin <- lm(Survived ~ Pclass, data = titanic)

hist(resid(m_lin))

```

Las **funciones de enlace** sirven para linealizar la relación entre la variable respuesta y la independiente. Para cada distribución de la variable respuesta y, por tanto de sus residuos, hay funciones de enlace por defecto (canónicas).

![Funciones de enlace canónicas para diferentes distribuciones de errores.](images/clipboard-746937596.png)

```{r binomial}
#| warning: false

# install.packages("DHARMa")
library(DHARMa)

ggplot(titanic) +
  geom_count(aes(x = Pclass, y = Survived))

m_binomial <- glmmTMB(Survived ~ Pclass, data = titanic, family = "binomial")

summary(m_binomial)

# La estimación de los parámetros (= probabilidad de supervivencia) está en escala logit. logit(p) = ln(p / (1-p))

# Intercepto: probabilidad de supervivencia del nivel de referencia (primera clase)

co_binomial <- fixef(m_binomial)
plogis(co_binomial[[1]][1])

titanic |> 
  group_by(Pclass) |> 
  mutate(n_class = n()) |> 
  group_by(Pclass, n_class, Survived) |> 
  summarise(n_sur = n()) |> 
  mutate(per_sur = n_sur/n_class)

# Para el resto de parámetros:

plogis(co_binomial[[1]][1]+co_binomial[[1]][2])

plot_model(m_binomial, type = "pred")
tidy(m_binomial)

simulateResiduals(m_binomial, plot = TRUE)
# DHARMa simula residuos utilizando las predicciones del modelo para generar múltiples conjuntos simulados de valores de respuesta, basándose en la distribución estadística asumida, en este caso y ~ binomial(n, p). Los residuos de DHARMa son redisuos simulados entre el nº de simulaciones, lo que hace que tome valores entre 0 y 1 a lo largo de toda la variable predictora.

# Sobredispersión: varianza residual entre grados de libertad de la varianza residual. Debería estar en torno a uno. Zuur et al. 2013 (A beginner’s guide to GLM and GLMM with R. A frequentist and Bayesian persperctive for ecologists.)

e <- resid(m_binomial, type = "pearson")
n <- nrow(m_binomial$frame)
p <- length(fixef(m_binomial))
sum(e^2)/(n-p)

check_overdispersion(m_binomial)

```

## Bibliografía de interés
